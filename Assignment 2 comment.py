# -*- coding: utf-8 -*-
"""assign2_abhishek_personal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jPH5SIbPYJGGYCyYQD_vBk4lWqB11v6K
"""

import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras.layers import Flatten, Dense

(X_train,y_train), (X_test,y_test) = keras.datasets.mnist.load_data() 
# we are loading the mnist dataset using this command and splitting them into test and training dataset
# in which traing will be done using the 60000 images and testing will be done using the remaining 10000 images.

print(X_train.shape) # this shows that the X_train has data of 60000 images all which are of 28 by 28 pixels
print(y_train.shape) # y_train has the lables for this these 60000 images that is if the given images represent 1 or 2 or 3.
print(X_test.shape) # this shows that the X_test has data of 10000 images all which are of 28 by 28 pixels
print(y_test.shape) # y_train has the lables for this these 10000 images.

X_train #here X_train is nothing but a collection of 60000 2d numpy arrays.

X_train[0]
# this is the 2d numpy array of the first image containing values between 0 and 255. 
#  Number closer to 0 states that the image is black on that side of the image and 
# more the number closer to 255, more the image is white in that portion. (0-255 are pixel values of the image)

plt.imshow(X_train[0])
# matplotlib has a function named as imshow() which creates an image from the given 2d array. 
# Here we are printing the first item from the training dataset
print(y_train[0]) 
# here we are printing the label for the corresponding image and same can be done for test dataset.

# It is always advised while training the neural networks to scale the data. 
# Scaling is used for making data points generalized so that the distance between them will be lower and that it fits within a specific scale like 0-1
# As we have data ranging fron 0-255, one method which coould be used is to divide the training and testing dataset by 255.
X_train = X_train/255
X_test = X_test/255

print(X_train[0]) #values of this matrix are now coverted in the range from 0-1.

model = Sequential()

model.add(Flatten(input_shape=(28,28))) 
# this is the input layer...it transforms the images of 2d array of 28x28 pixels into a 1d array of 784 pixels (28*28). 
# This layer has no parameters to learn as it only reformats the data and hence no activation function is used here.

model.add(Dense(128, activation = 'relu'))
# this is a hidden densly connected layer of 128 neurons.  Each neuron  takes input from all 784 neuron in the previous layer, 
# adjusting that input according to hidden parameters which will be learned during training, and gives output as single value to the next layer.

model.add(Dense(10, activation = 'softmax'))
# this is the output layer in which the above 128 neurons are followed by 10 neuron softmax layer and each of these neurons
# represents the class of mnist dataset. The output will be a value in the range [0, 1], representing the probability that the 
# image belongs to that class. The sum of all 10 neurons values is 1.

model.summary()

model.compile(optimizer='Adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history=model.fit(X_train,y_train, batch_size=50, epochs=10,verbose=True,validation_split=0.2)

model.evaluate(X_test,y_test) #using this step we evaluate the loss and accuracy of the model on our test data set.

predictions = model.predict(X_test) #in this step we store the predictions from our test dataset in the predeictions variable

# Commented out IPython magic to ensure Python compatibility.
# we will now plot the graph of Loss on training dataset vs the loss on VALIDATION DATASET

# %matplotlib inline
plt.title("Model Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.plot(history.history['loss'],color='red')
plt.plot(history.history['val_loss'])

# graph of training accuracy vs validation accuracy

plt.title("Model Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.plot(history.history['accuracy'],color='red')
plt.plot(history.history['val_accuracy'])

plt.plot(history.history['loss'],color= 'red')
plt.plot(history.history['accuracy'],color='green')

# prediction of the images

import random
n=random.randint(0,9999)

max_value = np.argmax(predictions[n])
print(max_value)
plt.imshow(X_test[n])

